{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T19:00:16.170087Z",
     "start_time": "2021-04-27T19:00:15.767554Z"
    }
   },
   "outputs": [],
   "source": [
    "import INN\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:26:56.449298Z",
     "start_time": "2021-04-25T20:26:56.436239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = INN.Sequential(INN.Nonlinear(dim=3, method='NICE'),\n",
    "                       INN.BatchNorm1d(3),\n",
    "                       INN.Nonlinear(dim=3, method='RealNVP'))\n",
    "model.eval()\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:26:37.191921Z",
     "start_time": "2021-04-25T20:26:37.186587Z"
    }
   },
   "outputs": [],
   "source": [
    "model = INN.BatchNorm1d(3)#INN.Nonlinear(dim=3, method='iResNet', num_n=100, num_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:26:38.913606Z",
     "start_time": "2021-04-25T20:26:38.908206Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_Jacobian_matrix(model, x):\n",
    "    batch_size, dim = x.shape\n",
    "    x.requires_grad = True\n",
    "    model.computing_p(True)\n",
    "    y, log_p, log_det = model(x)\n",
    "    \n",
    "    grad_list = []\n",
    "    for i in range(dim):\n",
    "        v = torch.zeros((batch_size, dim))\n",
    "        v[:, i] = 1\n",
    "        grad = INN.utilities.vjp(y, x, v)[0]\n",
    "        grad_list.append(grad.detach())\n",
    "    return torch.stack(grad_list, dim=1), log_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:26:39.163142Z",
     "start_time": "2021-04-25T20:26:39.156964Z"
    }
   },
   "outputs": [],
   "source": [
    "def Jacobian_matrix(model, x):\n",
    "    shape = x.shape\n",
    "    dim = int(torch.prod(torch.Tensor(list(x.shape))).item())\n",
    "    repeats = [dim]\n",
    "    for i in range(len(x.shape)):\n",
    "        repeats.append(1)\n",
    "    \n",
    "    x_hat = x.unsqueeze(0).repeat(tuple(repeats))\n",
    "    x_hat.requires_grad = True\n",
    "    model.computing_p(True)\n",
    "    y, log_p, log_det = model(x_hat)\n",
    "    \n",
    "    v = torch.diag(torch.ones(dim)).reshape((dim, *x.shape))\n",
    "    grad = INN.utilities.vjp(y, x_hat, v)[0]\n",
    "    \n",
    "    return grad.detach(), log_det.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:26:40.478075Z",
     "start_time": "2021-04-25T20:26:40.472116Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:26:58.975751Z",
     "start_time": "2021-04-25T20:26:58.958737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0207,  0.7147,  0.1159],\n",
       "        [ 0.3893,  1.9171, -0.6180],\n",
       "        [ 0.4169, -0.0570,  1.0998]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eps = 1e-5\n",
    "J, log_det = Jacobian_matrix(model, x)\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:27:00.256497Z",
     "start_time": "2021-04-25T20:27:00.250922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4258)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.abs(torch.det(J)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:27:00.812211Z",
     "start_time": "2021-04-25T20:27:00.802256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4258)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(log_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:27:57.577833Z",
     "start_time": "2021-04-25T20:27:57.573240Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn((6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:27:57.883569Z",
     "start_time": "2021-04-25T20:27:57.873327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J_g=tensor([ 1.9498, -0.0728,  1.3574,  0.7588,  1.3956,  0.7595]),\n",
      "J_c=tensor([ 1.9498, -0.0728,  1.3574,  0.7588,  1.3956,  0.7595])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "Js, log_det = linear_Jacobian_matrix(model, x)\n",
    "real_log_det = torch.log(torch.abs(torch.det(Js)))\n",
    "\n",
    "print(f'J_g={real_log_det},\\nJ_c={log_det.detach()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:28:05.686690Z",
     "start_time": "2021-04-25T20:28:05.681378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9760, -1.3065,  1.2205],\n",
       "        [ 0.5567, -0.8171, -1.6935],\n",
       "        [ 0.0597,  2.1638,  2.1138]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Js[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T05:04:11.505096Z",
     "start_time": "2021-04-25T05:04:11.498678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7945, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(-1 * torch.log(torch.var(x, dim=0, unbiased=False) + model.eps) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `INN.BatchNorm1d()` fails on Jacobian tests [fixed]\n",
    "2. `INN.iResNet()` has large differece to the ground-truth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T06:08:33.394253Z",
     "start_time": "2021-04-25T06:08:33.389501Z"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.BatchNorm1d(3, affine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T06:08:34.278052Z",
     "start_time": "2021-04-25T06:08:34.272396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T06:08:34.873855Z",
     "start_time": "2021-04-25T06:08:34.866593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3725, -0.3559, -1.4208],\n",
       "        [ 0.7690,  0.1532,  0.9104],\n",
       "        [ 0.9525, -0.5532, -0.3847],\n",
       "        [-2.0273,  0.2497,  1.6467],\n",
       "        [ 0.4455,  1.8891, -0.3031],\n",
       "        [ 0.2327, -1.3830, -0.4485]], grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T06:09:38.944101Z",
     "start_time": "2021-04-25T06:09:38.937688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3725, -0.3559, -1.4208],\n",
       "        [ 0.7690,  0.1532,  0.9104],\n",
       "        [ 0.9525, -0.5532, -0.3847],\n",
       "        [-2.0273,  0.2497,  1.6467],\n",
       "        [ 0.4455,  1.8891, -0.3031],\n",
       "        [ 0.2327, -1.3830, -0.4485]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = torch.var(x, dim=0, unbiased=False)\n",
    "mean = torch.mean(x, dim=0)\n",
    "\n",
    "(x - mean) / torch.sqrt(var + model.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:51:57.518139Z",
     "start_time": "2021-04-25T20:51:57.513012Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn((5, 3))\n",
    "bn = nn.BatchNorm1d(3, affine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:51:58.028429Z",
     "start_time": "2021-04-25T20:51:58.009478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6941,  0.2933, -0.2451],\n",
       "        [-0.1313, -0.2711,  1.4740],\n",
       "        [ 0.2754, -0.2282,  0.4445],\n",
       "        [ 0.1287, -1.4409, -0.0721],\n",
       "        [ 1.4213,  1.6469, -1.6014]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T17:12:07.640150Z",
     "start_time": "2021-04-27T17:12:07.636955Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn((3,3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T00:32:17.029541Z",
     "start_time": "2021-04-27T00:32:17.023938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T00:31:57.709249Z",
     "start_time": "2021-04-27T00:31:57.701963Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "attribute 'shape' of 'torch._C._TensorBase' objects is not writable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2924dc973659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: attribute 'shape' of 'torch._C._TensorBase' objects is not writable"
     ]
    }
   ],
   "source": [
    "x.shape = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T00:39:07.586468Z",
     "start_time": "2021-04-27T00:39:07.581351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3,4,5][4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T17:32:33.018092Z",
     "start_time": "2021-04-27T17:32:33.011748Z"
    }
   },
   "outputs": [],
   "source": [
    "class _default_1d_coupling_function(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, activation_fn=nn.ReLU, w=4):\n",
    "        super(_default_1d_coupling_function, self).__init__()\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise ValueError(f'kernel_size must be an odd number, but got {kernel_size}')\n",
    "        r = kernel_size // 2\n",
    "        \n",
    "        self.f = nn.Sequential(nn.Conv1d(channels, channels*w, kernel_size, padding=r),\n",
    "                               activation_fn(),\n",
    "                               nn.Conv1d(w*channels, w*channels, kernel_size, padding=r),\n",
    "                               activation_fn(),\n",
    "                               nn.Conv1d(w*channels, channels, kernel_size, padding=r)\n",
    "                              )\n",
    "    def forward(self, x):\n",
    "        return self.f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T17:57:38.517484Z",
     "start_time": "2021-04-27T17:57:38.506080Z"
    }
   },
   "outputs": [],
   "source": [
    "INNModule = INN.INNAbstract.INNModule\n",
    "\n",
    "class CouplingConv(INNModule):\n",
    "    '''\n",
    "    General invertible covolution layer for coupling methods\n",
    "    '''\n",
    "    def __init__(self, num_feature, mask=None):\n",
    "        super(CouplingConv, self).__init__()\n",
    "        self.num_feature = num_feature\n",
    "        if mask is None:\n",
    "            self.mask = self._mask(num_feature)\n",
    "        else:\n",
    "            self.mask = mask\n",
    "    \n",
    "    def _mask(self, n):\n",
    "        m = torch.zeros(n)\n",
    "        m[:(n // 2)] = 1\n",
    "        return m\n",
    "    \n",
    "    def working_mask(self, x):\n",
    "        '''\n",
    "        Generate feature mask for 1d inputs\n",
    "        x.shape = [batch_size, feature, *]\n",
    "        mask.shape = [1, feature, *(1)]\n",
    "        '''\n",
    "        batch_size, feature, *other = x.shape\n",
    "        mask = self.mask.reshape(1, self.num_feature, *[1] * len(other))\n",
    "        return mask\n",
    "\n",
    "\n",
    "class CouplingConv1d(CouplingConv):\n",
    "    '''\n",
    "    General 1-d invertible convolution layer for coupling methods\n",
    "    '''\n",
    "    def __init__(self, num_feature, mask=None):\n",
    "        super(CouplingConv1d, self).__init__(num_feature, mask=mask)\n",
    "\n",
    "\n",
    "class Conv1dNICE(CouplingConv1d):\n",
    "    '''\n",
    "    1-d invertible convolution layer by NICE method\n",
    "    '''\n",
    "    def __init__(self, channels, kernel_size, w=4, activation_fn=nn.ReLU, m=None, mask=None):\n",
    "        super(Conv1dNICE, self).__init__(num_feature=channels, mask=mask)\n",
    "        if m is None:\n",
    "            self.m = _default_1d_coupling_function(channels, kernel_size, activation_fn, w=w)\n",
    "        else:\n",
    "            self.m = m\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mask = self.working_mask(x)\n",
    "        \n",
    "        x_ = mask * x\n",
    "        x = x + (1-mask) * self.m(x_)\n",
    "        \n",
    "        x_ = (1-mask) * x\n",
    "        x = x + mask * self.m(x_)\n",
    "        return x\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        mask = self.working_mask(y)\n",
    "        \n",
    "        y_ = (1-mask) * y\n",
    "        y = y - mask * self.m(y_)\n",
    "        \n",
    "        y_ = mask * y\n",
    "        y = y - (1-mask) * self.m(y_)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def logdet(self, **args):\n",
    "        return 0\n",
    "\n",
    "\n",
    "class Conv1dNVP(CouplingConv1d):\n",
    "    '''\n",
    "    1-d invertible convolution layer by NICE method\n",
    "    TODO: inverse error is too large\n",
    "    '''\n",
    "    def __init__(self, channels, kernel_size, w=4, activation_fn=nn.ReLU, s=None, t=None, mask=None):\n",
    "        super(Conv1dNVP, self).__init__(num_feature=channels, mask=mask)\n",
    "        if s is None:\n",
    "            self.log_s = _default_1d_coupling_function(channels, kernel_size, activation_fn, w=w)\n",
    "        else:\n",
    "            self.log_s = s\n",
    "        \n",
    "        if t is None:\n",
    "            self.t = _default_1d_coupling_function(channels, kernel_size, activation_fn, w=w)\n",
    "        else:\n",
    "            self.t = t\n",
    "    \n",
    "    def s(self, x):\n",
    "        return torch.exp(self.log_s(x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mask = self.working_mask(x)\n",
    "        \n",
    "        x_ = mask * x\n",
    "        x = (1-mask) * (self.s(x_) * x + self.t(x_)) + x_\n",
    "        \n",
    "        mask = 1 - mask\n",
    "        x_ = mask * x\n",
    "        x = (1-mask) * (self.s(x_) * x + self.t(x_)) + x_\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        mask = 1 - self.working_mask(y)\n",
    "        \n",
    "        y_ = mask * y\n",
    "        y = (1-mask) * (y - self.t(y_)) / torch.exp(self.log_s(y_)) + y_\n",
    "        \n",
    "        mask = 1 - mask\n",
    "        y_ = mask * y\n",
    "        y = (1-mask) * (y - self.t(y_)) / torch.exp(self.log_s(y_)) + y_\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def logdet(self, **args):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:57:51.744941Z",
     "start_time": "2021-04-27T23:57:51.327686Z"
    }
   },
   "outputs": [],
   "source": [
    "import INN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:38:15.250404Z",
     "start_time": "2021-04-27T23:38:15.243695Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn((3, 5, 8)) * 10\n",
    "\n",
    "model = INN.Linear1d(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:38:19.391251Z",
     "start_time": "2021-04-27T23:38:19.374898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-4.5962e-01, -8.3237e-01, -5.2095e+00, -6.3192e+00, -9.2027e+00,\n",
       "           -2.3169e+01, -2.2931e+00, -1.5648e+00],\n",
       "          [ 2.1111e+00,  4.1545e+00, -4.4625e+00,  9.5212e+00,  4.8327e+00,\n",
       "            8.9690e+00, -2.5365e+01, -1.0859e+01],\n",
       "          [-3.9164e+00,  1.7029e+01,  4.7630e+00,  1.9899e+01, -2.3675e+01,\n",
       "           -5.6664e+00,  1.8262e+01,  4.9501e+00],\n",
       "          [ 3.7490e+00, -1.3335e+00,  3.8716e+00,  1.2091e+01, -2.0418e+00,\n",
       "           -1.1282e+01,  1.1244e+00, -9.5155e-01],\n",
       "          [ 2.0400e+00,  1.3096e+01, -4.2499e-01,  3.8104e+00, -9.6784e+00,\n",
       "            1.1031e+01,  6.2131e+00, -1.5430e+01]],\n",
       " \n",
       "         [[ 1.3582e+01, -1.9608e+01, -4.0115e+00,  1.2407e+00, -8.5277e+00,\n",
       "            7.0188e+00, -7.1900e+00, -1.7199e+00],\n",
       "          [-5.4620e+00, -9.5898e+00,  5.4368e+00,  2.3484e+00, -1.3124e+01,\n",
       "           -4.8956e+00, -2.5614e+00,  3.3316e+00],\n",
       "          [-3.0688e+00, -1.9365e-01, -4.9798e+00,  2.3397e+00,  1.3315e+01,\n",
       "           -4.1992e-01,  1.6362e+01, -2.1196e+00],\n",
       "          [ 6.8842e+00, -3.2285e+01,  6.9074e+00, -6.6047e+00,  6.8936e+00,\n",
       "            1.1748e+01,  1.1665e+01,  1.3624e-02],\n",
       "          [ 5.1480e-01, -2.3641e+00, -1.8759e+01,  4.9261e-01,  1.3590e+01,\n",
       "            1.3689e+01,  1.4643e+01, -6.3837e+00]],\n",
       " \n",
       "         [[ 3.0809e+00,  1.7189e+00,  6.9381e+00,  8.3455e+00, -2.0749e+01,\n",
       "           -1.3536e+01, -1.2680e+01,  2.4215e+00],\n",
       "          [ 8.7786e+00,  1.6969e+00,  1.2479e+00,  5.8923e-02,  5.5089e+00,\n",
       "            7.4215e+00, -3.0111e-01,  8.5233e+00],\n",
       "          [-1.5211e+01,  1.2217e+01, -8.2287e+00, -4.4192e+00,  2.9674e+00,\n",
       "           -4.8025e+00,  2.8082e+01, -2.1386e+01],\n",
       "          [ 8.0236e+00, -1.0300e+01,  6.9416e+00, -2.0048e+01,  1.2552e+01,\n",
       "            5.5606e+00, -9.4778e+00,  4.6419e+00],\n",
       "          [ 1.0716e+01,  6.8516e-01,  9.4519e+00,  4.1410e+00,  5.0152e+00,\n",
       "           -1.1848e+01,  8.3113e-01, -1.6843e+00]]], grad_fn=<SqueezeBackward1>),\n",
       " 0,\n",
       " tensor(-1.1921e-07, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T19:58:57.337371Z",
     "start_time": "2021-04-27T19:58:57.333241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17.2584,  8.3290, 27.7419], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T19:58:58.319863Z",
     "start_time": "2021-04-27T19:58:58.298200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3338, grad_fn=<L1LossBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.L1Loss()(model.inverse(y), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T19:59:29.710253Z",
     "start_time": "2021-04-27T19:59:29.690848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -5.9605e-08],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -4.7684e-07],\n",
       "         [-1.9073e-06, -2.3842e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 2.0160e+00,  4.5725e+00, -6.5988e+00, -6.9353e+00, -1.9093e+01,\n",
       "           6.4344e+00,  4.9652e+00, -9.1800e+00],\n",
       "         [-5.2022e+00, -1.6432e+01, -1.0714e+00,  3.1963e+01, -4.8019e+01,\n",
       "          -2.0962e+02, -1.3407e-02,  1.0146e+01],\n",
       "         [-3.1409e-01, -8.2552e+00, -7.9736e+00,  2.8125e+00,  1.7196e+01,\n",
       "          -1.2859e+01, -4.4796e+00, -1.7652e+00],\n",
       "         [-3.1207e+00, -2.1388e-01, -2.6068e+00,  1.2939e+00, -7.3147e+00,\n",
       "           1.0111e+01,  5.2085e-01,  1.9049e+00],\n",
       "         [-5.3876e+00,  4.4973e-01,  3.0528e+00,  4.2844e+00,  1.2069e+00,\n",
       "          -4.4137e-01,  3.5361e+00,  1.5774e+00]],\n",
       "\n",
       "        [[ 4.8114e+00,  1.2989e+02,  2.3822e-01,  7.6804e+00, -5.8815e-01,\n",
       "          -4.2366e+01, -4.9536e+00, -7.2709e+00],\n",
       "         [-3.7207e-01, -1.7346e+00,  7.8528e+01, -3.3229e+00, -1.4972e+02,\n",
       "           2.0417e+00, -2.4455e+00, -5.9568e+01],\n",
       "         [-1.4398e+00,  3.1692e+00, -2.1152e+00,  3.8740e+00, -1.5565e+01,\n",
       "           4.3865e+00,  1.1124e+01, -5.2823e+00],\n",
       "         [ 3.4121e+00, -2.6859e+00, -3.2128e+00,  3.5192e+00, -1.2694e+00,\n",
       "          -3.2050e+01,  6.2725e-01,  7.2276e+00],\n",
       "         [ 3.9963e+00,  6.1955e-01, -1.5151e+01, -1.3674e-01, -6.1263e+00,\n",
       "           8.4232e-01, -7.8503e+00, -3.9049e+00]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inverse(y) - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:26:21.259387Z",
     "start_time": "2021-04-27T23:26:19.896590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLUMatrix()"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = INN.utilities.PLUMatrix(5)\n",
    "mat.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:24:47.036842Z",
     "start_time": "2021-04-27T23:24:47.015982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 8])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = mat.W().unsqueeze(-1)\n",
    "F.conv1d(x, weight).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:24:53.132206Z",
     "start_time": "2021-04-27T23:24:53.123620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 8])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:26:26.893286Z",
     "start_time": "2021-04-27T23:26:26.591498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6578, -0.3593,  0.5535, -0.2352, -0.2766],\n",
       "        [-0.4351, -0.4065, -0.1952, -0.1574, -0.7633],\n",
       "        [ 0.2616, -0.7114, -0.5362,  0.1648,  0.3330],\n",
       "        [-0.1821, -0.0787, -0.0500, -0.9152,  0.3472],\n",
       "        [ 0.5258,  0.4398, -0.6046, -0.2349, -0.3309]], device='cuda:0',\n",
       "       grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.W()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:28:40.954636Z",
     "start_time": "2021-04-27T23:28:40.940067Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv1d1x1(nn.Module):\n",
    "    def __init__(self, num_feature, mat=None):\n",
    "        super(Conv1d1x1, self).__init__()\n",
    "        if mat is None:\n",
    "            self.mat = INN.utilities.PLUMatrix(num_feature)\n",
    "        else:\n",
    "            self.mat = mat\n",
    "    \n",
    "    def weight(self):\n",
    "        return self.mat.W().unsqueeze(-1)\n",
    "    \n",
    "    def weight_inv(self):\n",
    "        return self.mat.inv_W().unsqueeze(-1)\n",
    "    \n",
    "    def forward(self, x, log_p0=0, log_det_J=0):\n",
    "        return F.conv1d(x, self.weight())\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        return F.conv1d(y, self.weight_inv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:28:41.767634Z",
     "start_time": "2021-04-27T23:28:41.748974Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Conv1d1x1(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:28:49.569842Z",
     "start_time": "2021-04-27T23:28:49.559993Z"
    }
   },
   "outputs": [],
   "source": [
    "y = model(x).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:29:02.980919Z",
     "start_time": "2021-04-27T23:29:02.969234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.5367e-07, -7.7486e-07,  0.0000e+00, -2.5034e-06,  0.0000e+00,\n",
       "           1.0729e-06, -1.9073e-06,  5.3644e-07],\n",
       "         [-1.9073e-06,  9.5367e-07,  9.5367e-07,  2.8610e-06,  0.0000e+00,\n",
       "          -1.9073e-06,  1.9073e-06, -2.3842e-07],\n",
       "         [ 0.0000e+00,  0.0000e+00,  4.7684e-07,  0.0000e+00, -1.9073e-06,\n",
       "           0.0000e+00,  3.5763e-07, -9.5367e-07],\n",
       "         [-3.8147e-06,  0.0000e+00,  0.0000e+00,  1.9073e-06,  3.8147e-06,\n",
       "          -4.7684e-07,  0.0000e+00, -1.1921e-07],\n",
       "         [-1.6689e-06, -9.5367e-07,  4.7684e-07,  0.0000e+00,  4.7684e-07,\n",
       "           9.5367e-07, -1.9073e-06,  1.4305e-06]],\n",
       "\n",
       "        [[-7.1526e-07,  1.9073e-06,  4.7684e-07, -1.4305e-06,  0.0000e+00,\n",
       "           2.3842e-06, -1.4305e-06,  0.0000e+00],\n",
       "         [ 0.0000e+00,  9.5367e-07, -4.7684e-07, -2.6226e-06, -1.9073e-06,\n",
       "          -1.9073e-06,  9.5367e-07, -1.4305e-06],\n",
       "         [ 0.0000e+00,  0.0000e+00,  9.5367e-07,  1.9073e-06,  0.0000e+00,\n",
       "           4.7684e-07, -4.7684e-07,  4.7684e-07],\n",
       "         [-1.9073e-06, -4.7684e-07, -4.7684e-07, -1.9073e-06, -1.9073e-06,\n",
       "          -9.5367e-07, -5.9605e-08, -4.7684e-07],\n",
       "         [-9.5367e-07,  1.1921e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           9.5367e-07, -9.5367e-07, -1.3113e-06]],\n",
       "\n",
       "        [[-9.5367e-07,  0.0000e+00,  1.1921e-07,  0.0000e+00,  9.5367e-07,\n",
       "           1.9073e-06, -1.9073e-06,  4.7684e-07],\n",
       "         [ 1.3709e-06,  1.4305e-06,  9.5367e-07,  0.0000e+00,  0.0000e+00,\n",
       "           1.5497e-06,  9.5367e-07, -2.3842e-06],\n",
       "         [ 0.0000e+00,  0.0000e+00, -2.3842e-07,  9.5367e-07,  0.0000e+00,\n",
       "          -9.5367e-07,  0.0000e+00,  4.7684e-07],\n",
       "         [ 9.5367e-07,  0.0000e+00,  1.4305e-06, -2.3842e-07,  4.7684e-07,\n",
       "           1.9073e-06,  2.3842e-07, -1.9073e-06],\n",
       "         [-4.7684e-07,  1.1921e-07,  0.0000e+00,  9.5367e-07,  7.1526e-07,\n",
       "           1.6689e-06,  0.0000e+00,  1.7881e-07]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inverse(y) - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:40:47.700100Z",
     "start_time": "2021-04-27T23:40:47.693944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(5, 5, 1).weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T05:00:50.242305Z",
     "start_time": "2021-04-28T05:00:49.843971Z"
    }
   },
   "outputs": [],
   "source": [
    "import INN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T05:01:02.808084Z",
     "start_time": "2021-04-28T05:01:02.803244Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn((3, 5, 8))\n",
    "model = INN.PixelShuffle1d(2)\n",
    "model.computing_p(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T05:01:19.696452Z",
     "start_time": "2021-04-28T05:01:19.686536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inverse(model(x)) - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T01:18:20.763437Z",
     "start_time": "2021-04-28T01:18:20.758441Z"
    }
   },
   "outputs": [],
   "source": [
    "bnINN = INN.BatchNorm1d(5)\n",
    "bnINN.computing_p(False)\n",
    "bn = nn.BatchNorm1d(5, affine=False)\n",
    "\n",
    "#bnINN.eval()\n",
    "#bn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T01:18:37.776361Z",
     "start_time": "2021-04-28T01:18:37.765331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnINN(x) - bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T01:03:10.360320Z",
     "start_time": "2021-04-28T01:03:10.353978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnINN.running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T01:23:49.280424Z",
     "start_time": "2021-04-28T01:23:49.271003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3992, 0.3992, 0.3992, 0.3992, 0.3992])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1) * torch.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T01:45:29.761813Z",
     "start_time": "2021-04-28T01:45:29.753378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PixelShuffle in module torch.nn.modules.pixelshuffle:\n",
      "\n",
      "class PixelShuffle(torch.nn.modules.module.Module)\n",
      " |  PixelShuffle(upscale_factor: int) -> None\n",
      " |  \n",
      " |  Rearranges elements in a tensor of shape :math:`(*, C \\times r^2, H, W)`\n",
      " |  to a tensor of shape :math:`(*, C, H \\times r, W \\times r)`, where r is an upscale factor.\n",
      " |  \n",
      " |  This is useful for implementing efficient sub-pixel convolution\n",
      " |  with a stride of :math:`1/r`.\n",
      " |  \n",
      " |  See the paper:\n",
      " |  `Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network`_\n",
      " |  by Shi et. al (2016) for more details.\n",
      " |  \n",
      " |  Args:\n",
      " |      upscale_factor (int): factor to increase spatial resolution by\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(*, C_{in}, H_{in}, W_{in})`, where * is zero or more batch dimensions\n",
      " |      - Output: :math:`(*, C_{out}, H_{out}, W_{out})`, where\n",
      " |  \n",
      " |  .. math::\n",
      " |      C_{out} = C_{in} \\div \\text{upscale\\_factor}^2\n",
      " |  \n",
      " |  .. math::\n",
      " |      H_{out} = H_{in} \\times \\text{upscale\\_factor}\n",
      " |  \n",
      " |  .. math::\n",
      " |      W_{out} = W_{in} \\times \\text{upscale\\_factor}\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> pixel_shuffle = nn.PixelShuffle(3)\n",
      " |      >>> input = torch.randn(1, 9, 4, 4)\n",
      " |      >>> output = pixel_shuffle(input)\n",
      " |      >>> print(output.size())\n",
      " |      torch.Size([1, 1, 12, 12])\n",
      " |  \n",
      " |  .. _Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network:\n",
      " |      https://arxiv.org/abs/1609.05158\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PixelShuffle\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, upscale_factor: int) -> None\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  forward(self, input: torch.Tensor) -> torch.Tensor\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'upscale_factor': <class 'int'>}\n",
      " |  \n",
      " |  __constants__ = ['upscale_factor']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.PixelShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
